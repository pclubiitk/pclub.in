<!DOCTYPE html>
<html>

  <head>
  <!---
  Dark Kimochi By Spaghetti
  -->
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
  <title>Variational Autoencoders</title>
  <meta name="description" content="Introduction">
  <meta name="google-site-verification" content="Jo2tsvJukG-gUtQpsqsSJRDXKo5XD10xutu1oJd-eTU" />
  <meta name="msvalidate.01" content="39EAB6293E810C6EFD770A53ECE7C7FE" />
  <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/assets/css/flex.css">
  <link rel="stylesheet" href="/assets/css/alternative.css">
  <link href="//cdnjs.cloudflare.com/ajax/libs/typicons/2.0.7/typicons.min.css" rel="stylesheet">
  <link href="//cdnjs.cloudflare.com/ajax/libs/octicons/3.4.1/octicons.min.css" rel="stylesheet">
  <link rel="canonical" href="http://pclub.in/probabilistic%20machine%20learning/generative%20networks/2019/04/10/variational-autoencoder.html">
  <link rel="alternate" type="application/rss+xml" title="Programming Club IIT Kanpur" href="http://pclub.in/feed.xml">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/image/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/image/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/image/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/image/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/image/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/image/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/image/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/image/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/image/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/image/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/image/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/image/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/image/favicon-16x16.png">
  <link rel="manifest" href="/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/image/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
</head>


  <Body>

    <header class="site-header">
  <div class="wrapper">
<div class="pacel"><div class="pace-progress" data-progress-text="100%" data-progress="99" style="transform: translate3d(100%, 0px, 0px);">
  <div class="pace-progress-inner"></div>
</div>
<div class="pace-activity"></div></div>
  <a id="route" class="site-title" href="/">Programming Club IIT Kanpur</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#fff" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#fff" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#fff" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          
          
        
          
        
          
          
          <a id="route" class="page-link" href="/about.html">About</a>
          
          
        
          
          
          <a id="route" class="page-link" href="/archive.html">Archive</a>
          
          
        
          
          
          <a id="route" class="page-link" href="/contact.html">Contact</a>
          
          
        
          
        
          
        
          
        
          
        
          
        
          
          
          <a id="route" class="page-link" href="/projects.html">Projects</a>
          
          
        
          
        
          
          
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <!--<a class="page-link" href="http://pclub.in/feed.xml"> Feeds</a>-->
        <a class="page-link" href="http://github.com/pclubiitk?tab=repositories"><i class="typico typcn typcn-social-github" style="font-size:"></i> Repositories</a>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div style="margin: 4.5rem auto;">

<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="inpost-header">
    <h1 class="inpost-title" itemprop="name headline">Variational Autoencoders</h1>
    <h1 style="float:right" class="comment-postingatas">
      <i class="typcn typcn-messages"></i> <a class="disqus-comment-count" href="/probabilistic%20machine%20learning/generative%20networks/2019/04/10/variational-autoencoder.html#disqus_thread" data-disqus-identifier="/probabilistic%20machine%20learning/generative%20networks/2019/04/10/variational-autoencoder"style="color:#fff"> 0</a>
    </h1>
    </p>
  </header>
  
  <div class="post-content" itemprop="articleBody">
    <h3 id="introduction">Introduction</h3>

<p>Neural networks , especially convolutional neural networks, have a lot to offer than classification tasks and its optimization. Variational autoencoders are an example of a use case of CNN lets you generate new images by learning the probability distribution of the images of the input dataset . Although they are currently inferior to the state of the art networks for similar usecase, i.e GANs ,  a fully probabilistic approach for Variational autoencoders make them quite interesting to explore.</p>

<h3 id="prerequisites">Prerequisites</h3>
<ul>
  <li><strong>Elementary</strong> knowledge about probability distributions and convolutional neural networks</li>
  <li>Desire to learn about generative networks</li>
</ul>

<p>Let us begin. We’ll start with a deep dive into pros and cons of autoencoders and KL divergence .</p>

<h3 id="autoencoders">Autoencoders</h3>

<p>An autoencoder network is a pair of two connected networks- an encoder and a decoder. An encoder network takes in an input, and converts it into a smaller, dense representation, which the decoder network can use to convert it back to the 
original input.
Usually , the convolutional layers of any CNN take in a large image (eg. rank 3 tensor of size 28x28x3), and convert it to a much more compact, dense representation (eg. rank 1 tensor of size 10) which is then used by the fully connected classifier network to classify the image. Similary, the encoder network takes in an input and produces a much smaller representation that contains enough information for the decoder network to process and convert it into the desired output format. 
Autoencoders take this idea, and slightly flip it on its head, by making the encoder generate encodings specifically useful for reconstructing its own input.</p>

<p>The entire network is usually trained as a whole. The loss function is usually either the mean-squared error or cross-entropy between the output and the input, known as the reconstruction loss, which penalizes the network for creating outputs different 
from the input.</p>

<p>As the encoding has far less units than the input, the encoder must choose to discard information. The encoder learns to preserve as much of the relevant information as possible and discards the irrelevant stuff. The decoder learns to take the encoding and reconstruct it into a proper image. Together, they form an autoencoder network which indeed is quite successful in reconstructing the input data .</p>

<p><img src="https://www.oreilly.com/library/view/neural-network-programming/9781788390392/assets/f98d20e7-7c2e-4a37-8e18-5fc0bc23ab87.png" alt="image" /></p>

<h3 id="why-vartional-autoencoders">Why Vartional Autoencoders?</h3>

<p>Standard autoencoders learn to generate compact representations and reconstruct their inputs well to a good accuracy level but the input data does not cover the entire latent space (where encoded vectors lie) . Due to this discontinuous latent space, the purpose of generating new data is not served by simple autoencoders .
For example, training an autoencoder on the MNIST dataset, and visualizing the encodings from a 2D latent space reveals the formation of distinct clusters. This makes sense, as distinct encodings for each image type makes it far easier for the decoder to decode them. This is fine if you’re just replicating the same images.
But when you’re building a generative model, you don’t want to prepare to replicate the same image you put in. You want to randomly sample from the latent space, or generate variations on an input image, from a continuous latent space.
If the space has discontinuities (eg. gaps between clusters) and you sample/generate a variation from there, the decoder will simply generate an unrealistic output, because the decoder has no idea how to deal with that region of the latent space.</p>

<p>Variational Autoencoders (VAEs) have one fundamentally unique property that differentiates them from autoencoders : their latent spaces are, by design, continuous, allowing easy random sampling .This property that makes them so useful and accurate for generating data</p>

<h3 id="kl-divergence">KL divergence</h3>

<p>In this post we’re going to take a look at way of comparing two probability distributions called Kullback-Leibler Divergence (often shortened to just KL divergence). Very often in Probability and Statistics, we’ll replace observed data or a complex distributions with a simpler, approximating distribution. KL Divergence helps us to measure just how much information we lose when we choose an approximation.
KL Divergence has its origins in information theory. The primary goal of information theory is to measure how much information is in data. The most important metric in information theory is called Entropy, typically denoted as <strong>H</strong>. Entropy of a probability distribution is defined as following :
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/527fa6ed7da2d6fcfb64cc71b4fc09b4c248887a" alt="image" /><br />
 ,where common values of b are 2 or e and P(X) denotes the Probability mass function .</p>

<p>Kullback-Leibler Divergence is just a slight modification of our formula for entropy. Rather than just having our probability distribution <strong>P</strong> ,we measure the difference of the log values of the approximating probability distribution <strong>Q</strong>
 <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/726edcd02293461b82768ea2fd299c3a3ef16112" alt="image" />   for discrete probability distributions <br />
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/756dd25036c5da76a59e58a001f3196e059f537d" alt="image" />   for continuous probability distributions <br />
Relating this with entropy,
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3576ec5ae54b2b2df156b620d4982009c0a73432" alt="image" /><br />
where <strong>H(P,Q)</strong> is the cross entropy of <strong>P</strong> and <strong>Q</strong>.</p>

<p>We can observe here that by  minimizing the KL divergence of two probability distributions , we can make our approximating distribution more closer to the actual probability distribution and this optimization is tractable .Combining KL divergence with neural networks allows us to learn very complex approximating distribution for our data and this approach is used Variational  Autoencoders .</p>

<h3 id="deep-dive-into-vaes">Deep dive into VAEs</h3>
<p>The unique property in VAEs which makes them so useful in maing generative models is that their latent space is continuous which allows random sampling .This happens because its encoder does not output an encoding vector of size n, rather, outputs two vectors of size n: a vector of means, <strong>μ</strong>, and another vector of standard deviations, <strong>σ</strong> . <strong>μ</strong> and <strong>σ</strong> now become the parameters for a vector of size n containing random values having mean and standard deviation as <strong>μ</strong> and <strong>σ</strong> respectively . This will result in random generation of data having mean and standard deviation close to the input data.<br />
<img src="https://cdn-images-1.medium.com/max/1600/1*96ho7qSyW0nKrLvSoZHOtA.png" alt="image" />  <br />
Hence,the mean vector controls where the encoding of an input should be centered around, while the standard deviation controls the area, how much from the mean the encoding can vary. As encodings are generated at random from anywhere inside the shaded portion, the decoder learns all the nearby points that refer to sample of that class . This allows the decoder to not just decode specific encodings in the latent space but also the ones which vary thus making the latent space continuous .</p>

<p><img src="https://cdn-images-1.medium.com/max/1600/1*xCjoga9IPyNUSiz9E7ao7A.png" alt="image" /><br />
We want our encodings to be as close as possible while still being distinct to make smooth interpolation, but sometimes we might end up making our latent space similar to the right picture shown above as there are no limits on what values vectors μ and σ can take on, so to avoid this , we introduce KL divergence in the loss function . 
The following image shows the calculation of the log of the data likelihood<br />
<img src="https://github.com/namanbiyani/images/blob/master/Screenshot%20from%202019-04-16%2010-30-17.png?raw=true" alt="image" /></p>

<table>
  <tbody>
    <tr>
      <td>Although the third term in intractable, by Gibbs inequality , we know that KL divergence of two distributions is always non-negative . Hence, the first two terms can be a tractable lower bound to the log data likelihood which we can easily differentiate (P(X</td>
      <td>Z) is mostly a simple distribution like Guassian and KL term is also differntiable ) and hence optimize .The combination of first two terms is also known as Variational Lower Bound (“ELBO”) . The first term here is responsible for reconstructing the original input data and the second term is responsible for making approximate distribution close to input distribution .</td>
    </tr>
  </tbody>
</table>

<h3 id="applications-of-vaes">Applications of VAEs</h3>
<ol>
  <li>One can generate fake human face photos as accurate as the following<br />
<img src="https://cdn-images-1.medium.com/max/1600/1*EhsiaTuiKtvuRDe_wBRX5Q.png" alt="image" /></li>
  <li>VAEs are highly successful in Vector arithmetic . For example, if we would like to add features like adding glasses on a face ,we can find two samples, one with glasses and one without, obtain their encoded vectors from the encoder, and save the difference. Add this new “glasses” vector to any other face image and decode it.<br />
<img src="https://cdn-images-1.medium.com/max/1200/1*YTV9WARg5fdQ2jKieExb0A.png" alt="image" /><img src="https://cdn-images-1.medium.com/max/1200/1*El2DhlTK5duHyVxVbdqk9Q.png" alt="image" /></li>
  <li>One could even train an autoencoder using LSTM encoder-decoder pairs for sequential, discrete data (something not possible with methods such as GANs), to produce synthetic text, or even interpolate between MIDI samples such as Google Brain’s Magenta’s MusicVAE .</li>
</ol>

<h3 id="supporting-links">Supporting Links</h3>

<p>So I guess this is enough for a start. There is much to learn, but perhaps that would be covered in another article. I present to you a few useful links:</p>

<ul>
  <li><a href="https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/">Tutorial on Variational autoencoder</a></li>
  <li><a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">KL divergence blog</a></li>
  <li>Do read research papers related to this as literature survey is highly important to get a feel of it.</li>
</ul>


  </div>

<div class="arrowNav">
 
 
<div class="arrowLeft">
<a id="route" class="tooltip-right" href="/cryptography/2019/04/15/DES.html" title="
&lt;p class=hoveratas&gt;Data Encryption Standard&lt;/p&gt; &lt;p class=hoverbawah&gt;Data encryption standard is a **symmetric-key block cipher** published by...&lt;/p&gt;
"><i class="typcn typcn-chevron-left" style="    font-size: 30px;
    position: relative;
   
    "></i></a>
</div>


 
<div class="arrowRight" style="float:right">
<a id="route" class="tooltip-left" href="/pclub/2017/08/11/introworkshop17.html" title="
&lt;p class=hoveratas&gt;Introductory Workshop 2017&lt;/p&gt; &lt;p class=hoverbawah&gt;Introduction Hi all, welcome to the Programming Club Introductory Workshop...&lt;/p&gt;
"><i class="typcn typcn-chevron-right" style="    font-size: 30px;
    position: relative;
   
    "></i></a>
</div>

</div>


  <p class="post-meta" style="background: #00bc8c; padding: 15px 0px 15px 20px;">
  <time datetime="2019-04-10T00:00:00+00:00" itemprop="datePublished"><i class="typcn typcn-calendar-outline"></i> Apr 10, 2019</time>
   
  • <span itemprop="author" itemscope itemtype="http://schema.org/Person">
    <span itemprop="name">
      
      Naman Biyani
      
    </span>
  </span>
   
    |
  
  <a href="/tags.html#probabilistic machine learning" style="color: #fff;"><i class="typcn typcn-tags"></i> probabilistic machine learning</a>
  
  <a href="/tags.html#probability distributions" style="color: #fff;"><i class="typcn typcn-tags"></i> probability distributions</a>
  
  <a href="/tags.html#optimizing divergences" style="color: #fff;"><i class="typcn typcn-tags"></i> optimizing divergences</a>
  
  </p>

</article>





<style>
    #disqus_thread {
  overflow: hidden;

  iframe {
    margin-bottom: -54px;
  }
}
</style>
<div id="disqus_thread"></div>
<script>
  // For debugging
  // var disqus_developer = 1;
  var disqus_shortname = 'pclubiitk';
  var disqus_config = function () {
    this.page.url = 'https://pclub.in/probabilistic%20machine%20learning/generative%20networks/2019/04/10/variational-autoencoder';
    this.page.identifier = '/probabilistic%20machine%20learning/generative%20networks/2019/04/10/variational-autoencoder';
  };
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  (function () {
    var s = document.createElement('script'); s.async = true;
    s.type = 'text/javascript';
    s.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
  }());
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

</div>

      </div>
    </div>

    
<footer class="site-footer">
  <div class="wrapper">
    <h2 class="footer-heading">Programming Club IIT Kanpur</h2>
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>The Programming Club, IIT Kanpur
</li>
          <li><a href="mailto:pclubiitk@gmail.com">pclubiitk@gmail.com</a></li>
          <li>Send us Pull Requests to this website <a href="https://github.com/pclubiitk/pclub.in" target="_blank">here</a></li>
        </ul>
      </div>
      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/pclubiitk"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">pclubiitk</span></a>

          </li>
          
          
          
          <li>
            <a href="https://facebook.com/groups/pclubiitk"><span class="icon icon--facebook"><?xml version="1.0"?>
<svg class="icon icon-facebook" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16">
  <path class="background" fill="#828282" d="M 0.875,0 C 0.38722,0 0,0.38721 0,0.875 l 0,14.25 C 0,15.61266 0.38715,16 0.875,16 l 7.65625,0 0,-6.1875 -2.0625,0 0,-2.4375 2.0625,0 0,-1.78125 c 0,-2.06656 1.2814395,-3.1875 3.125,-3.1875 0.88307,0 1.62261,0.06435 1.84375,0.09375 l 0,2.15625 -1.25,0.03125 c -1.00252,0 -1.21875,0.45713 -1.21875,1.15625 l 0,1.53125 2.40625,0 -0.3125,2.4375 -2.09375,0 0,6.1875 4.09375,0 C 15.612599,16 16,15.61272 16,15.125 L 16,0.875 C 16,0.38721 15.61266,0 15.125,0 L 0.875,0 z"/>
  <path class="foreground" fill="white" d="m 11.03968,16.000001 v -6.19608 h 2.07973 l 0.31141,-2.41472 h -2.39114 v -1.54165 c 0,-0.69912 0.19414,-1.17555 1.19666,-1.17555 l 1.27866,-5.6e-4 v -2.15974 c -0.22114,-0.0294 -0.98017,-0.0952 -1.86324,-0.0952 -1.8435605,0 -3.1057105,1.12532 -3.1057105,3.19188 v 1.7808 h -2.08506 v 2.41472 h 2.08506 v 6.19608 H 11.03968 z"/>
</svg>
</span><span class="username">PClubIITK</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/pclubiitk"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">pclubiitk</span></a>

          </li>
          
        </ul>
      </div>
      <div class="footer-col footer-col-3">
        <p style="font-size: xx-small;">Design by Spaghetti</p>
      </div>
    </div>
  </div>
</footer>

<!-- Loading JavaScript -->
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
<script type="text/javascript" src="/assets/js/jquery-preload.js"></script>
<script type="text/javascript" src="/assets/js/jquery-tooltipster.js"></script>
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script>
<script type="text/javascript" src="/assets/js/jquery-autocomplete.js"></script>
<script type="text/javascript" src="/assets/js/responsive_waterfall.js"></script>
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script>

<script>
$(document).ready(function() {
    "/" != location.pathname ? $('.trigger a[href^="/' + location.pathname.split("/")[1] + '"]').addClass("active") : $(".none a:eq(0)").addClass("active")
}), $(document).ready(function() {
    $(".tooltip-right").tooltipster({
        contentAsHTML: !0,
        position: "right"
    })
}), $(document).ready(function() {
    $(".tooltip-left").tooltipster({
        contentAsHTML: !0,
        position: "left"
    })
});
$(document).ready(function() {
    $(".menu-icon").click(function() {
        $(".trigger").toggle()
    })
}), Pace.on("done", function() {
    $(".pacel").hide()
}), $(function() {
    $("img").lazyload({})
});
var options = {
    url: "/gblk.json",
    getValue: "title",
    list: {
        match: {
            enabled: !0,
            maxNumberOfElements: 5
        }
    },
    template: {
        type: "links",
        fields: {
            link: "url"
        }
    },
    theme: "square"
};
$("#countries").easyAutocomplete(options);
var waterfall = new Waterfall({
    containerSelector: ".wf-container",
    boxSelector: ".wf-box",
    minBoxWidth: 180
});
</script>


  </Body>

</html>
